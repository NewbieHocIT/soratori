{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyP9z2j6eFn2+PBohufLpjqS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NewbieHocIT/soratori/blob/main/speech_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets librosa numpy hmmlearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "TJYRLjjVxqNy",
        "outputId": "792c537d-1315-4626-ea4e-175993aafd8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.10/dist-packages (0.10.2.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: hmmlearn in /usr/local/lib/python3.10/dist-packages (0.3.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.4.2)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.13.0)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (4.12.2)\n",
            "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa) (4.3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2024.10.0\n",
            "    Uninstalling fsspec-2024.10.0:\n",
            "      Successfully uninstalled fsspec-2024.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.2.0 dill-0.3.8 fsspec-2024.9.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Kết nối Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOJkOtyExtkr",
        "outputId": "aac3c290-f287-40a4-e6bb-63289432ccf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nddhjAR1yYKi",
        "outputId": "baf1c6b0-4f69-420c-b4e0-8de956359daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `SoraToris` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `SoraToris`\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install jiwer\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzVLVtWq3oTT",
        "outputId": "3ff65e0b-939f-48d7-f399-6375b6e8cb62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (8.1.8)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading jiwer-3.0.5-py3-none-any.whl (21 kB)\n",
            "Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.5 rapidfuzz-3.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import librosa\n",
        "from hmmlearn import hmm\n",
        "from datasets import load_dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Bước 1: Tải dataset từ Hugging Face\n",
        "def load_common_voice(language=\"vi\", split=\"train\"):\n",
        "    \"\"\"\n",
        "    Tải dataset Common Voice từ Hugging Face.\n",
        "    \"\"\"\n",
        "    print(\"Đang tải dataset...\")\n",
        "    dataset = load_dataset(\"mozilla-foundation/common_voice_13_0\", language, split=split)\n",
        "    audio_files = []\n",
        "    transcripts = []\n",
        "    for entry in dataset:\n",
        "        audio_files.append(entry[\"audio\"][\"path\"])\n",
        "        transcripts.append(entry[\"sentence\"])\n",
        "    return audio_files, transcripts\n",
        "\n",
        "# Bước 2: Tạo bộ tokenizer (dựa trên ký tự)\n",
        "class CharacterTokenizer:\n",
        "    def __init__(self):\n",
        "        self.encoder = None\n",
        "        self.decoder = None\n",
        "\n",
        "    def fit(self, sentences):\n",
        "        \"\"\"\n",
        "        Tạo encoder và decoder từ danh sách các câu.\n",
        "        \"\"\"\n",
        "        unique_chars = set(\"\".join(sentences))  # Tập hợp các ký tự duy nhất\n",
        "        self.encoder = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n",
        "        self.decoder = {idx: char for char, idx in self.encoder.items()}\n",
        "\n",
        "    def encode(self, sentence):\n",
        "        \"\"\"\n",
        "        Mã hóa một câu thành token ID.\n",
        "        \"\"\"\n",
        "        return [self.encoder[char] for char in sentence]\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"\n",
        "        Giải mã token ID thành câu văn bản.\n",
        "        \"\"\"\n",
        "        return \"\".join([self.decoder[token_id] for token_id in token_ids])\n",
        "\n",
        "# Bước 3: Trích xuất đặc trưng MFCC từ âm thanh\n",
        "def extract_features(file_path, n_mfcc=13):\n",
        "    \"\"\"\n",
        "    Trích xuất đặc trưng MFCC từ một tệp âm thanh.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        y, sr = librosa.load(file_path, sr=None)  # Tải tệp âm thanh\n",
        "        mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)  # Trích xuất đặc trưng MFCC\n",
        "        return mfcc.T  # Chuyển vị để phù hợp với HMM\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi trích xuất đặc trưng từ {file_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Bước 4: Huấn luyện mô hình HMM\n",
        "def train_hmm(features_list, n_components=5):\n",
        "    \"\"\"\n",
        "    Huấn luyện mô hình HMM với các đặc trưng đã trích xuất.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        all_features = np.concatenate(features_list, axis=0)\n",
        "        model = hmm.GaussianHMM(n_components=n_components, covariance_type=\"diag\", n_iter=100)\n",
        "        model.fit(all_features)\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Lỗi khi huấn luyện HMM: {e}\")\n",
        "        return None\n",
        "\n",
        "# Bước 5: Chuẩn bị hệ thống Speech-to-Text\n",
        "if __name__ == \"__main__\":\n",
        "    # Đường dẫn tạm thời để kiểm tra\n",
        "    os.environ[\"HF_HOME\"] = \"./huggingface\"\n",
        "\n",
        "    # Tải dữ liệu Common Voice\n",
        "    language = \"vi\"  # Tiếng Việt\n",
        "    audio_files, transcripts = load_common_voice(language=language, split=\"train\")\n",
        "\n",
        "    # Kiểm tra dữ liệu đã tải\n",
        "    print(f\"Tổng số mẫu dữ liệu: {len(audio_files)}\")\n",
        "\n",
        "    # Tạo bộ tokenizer\n",
        "    tokenizer = CharacterTokenizer()\n",
        "    tokenizer.fit(transcripts)\n",
        "\n",
        "    # Kiểm tra tokenizer\n",
        "    sample_sentence = transcripts[0]\n",
        "    encoded = tokenizer.encode(sample_sentence)\n",
        "    decoded = tokenizer.decode(encoded)\n",
        "    print(f\"Ví dụ token hóa:\\n- Gốc: {sample_sentence}\\n- Mã hóa: {encoded}\\n- Giải mã: {decoded}\")\n",
        "\n",
        "    # Huấn luyện mô hình HMM cho từng câu\n",
        "    models = {}\n",
        "    for i, (audio_file, transcript) in enumerate(zip(audio_files[:20], transcripts[:20])):  # Chỉ dùng 10 tệp đầu tiên\n",
        "        print(f\"Đang xử lý tệp {i+1}/{10}: {audio_file}\")\n",
        "        features = extract_features(audio_file)\n",
        "        if features is not None:\n",
        "            model = train_hmm([features])\n",
        "            if model:\n",
        "                token_ids = tokenizer.encode(transcript)  # Mã hóa câu văn bản\n",
        "                models[tuple(token_ids)] = model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kFew3tN94J0o",
        "outputId": "e253ae76-295f-40dc-fe8f-5a19d9c282e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Đang tải dataset...\n",
            "Tổng số mẫu dữ liệu: 2462\n",
            "Ví dụ token hóa:\n",
            "- Gốc: quả nhiên trúng tuyển vào trường Quốc Lập\n",
            "- Mã hóa: [45, 49, 86, 0, 42, 37, 38, 64, 42, 0, 48, 46, 72, 42, 36, 0, 48, 49, 52, 103, 42, 0, 50, 58, 43, 0, 48, 46, 83, 118, 42, 36, 0, 23, 49, 111, 33, 0, 19, 92, 44]\n",
            "- Giải mã: quả nhiên trúng tuyển vào trường Quốc Lập\n",
            "Đang xử lý tệp 1/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901117.mp3\n",
            "Đang xử lý tệp 2/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901118.mp3\n",
            "Đang xử lý tệp 3/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901119.mp3\n",
            "Đang xử lý tệp 4/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901120.mp3\n",
            "Đang xử lý tệp 5/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901121.mp3\n",
            "Đang xử lý tệp 6/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901282.mp3\n",
            "Đang xử lý tệp 7/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901283.mp3\n",
            "Đang xử lý tệp 8/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901285.mp3\n",
            "Đang xử lý tệp 9/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901286.mp3\n",
            "Đang xử lý tệp 10/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901288.mp3\n",
            "Đang xử lý tệp 11/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901300.mp3\n",
            "Đang xử lý tệp 12/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901301.mp3\n",
            "Đang xử lý tệp 13/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901302.mp3\n",
            "Đang xử lý tệp 14/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901304.mp3\n",
            "Đang xử lý tệp 15/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901305.mp3\n",
            "Đang xử lý tệp 16/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901408.mp3\n",
            "Đang xử lý tệp 17/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901409.mp3\n",
            "Đang xử lý tệp 18/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901412.mp3\n",
            "Đang xử lý tệp 19/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901474.mp3\n",
            "Đang xử lý tệp 20/10: /root/.cache/huggingface/datasets/downloads/extracted/58399062ad245dfaf8f114ef10011bc6da77257ce12add144a19ecc9c0bfcb6d/vi_train_0/common_voice_vi_23901475.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kiểm tra bộ tokenizer\n",
        "print(\"Bộ mã hóa (Encoder):\")\n",
        "for char, idx in tokenizer.encoder.items():\n",
        "    print(f\"{char}: {idx}\")\n",
        "\n",
        "print(\"\\nBộ giải mã (Decoder):\")\n",
        "for idx, char in tokenizer.decoder.items():\n",
        "    print(f\"{idx}: {char}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JM7mO99j2_rO",
        "outputId": "8458ae38-4048-4bce-c9ba-34755143aa87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bộ mã hóa (Encoder):\n",
            " : 0\n",
            "!: 1\n",
            "\": 2\n",
            "': 3\n",
            ",: 4\n",
            "-: 5\n",
            ".: 6\n",
            ":: 7\n",
            ";: 8\n",
            "?: 9\n",
            "A: 10\n",
            "B: 11\n",
            "C: 12\n",
            "D: 13\n",
            "E: 14\n",
            "F: 15\n",
            "G: 16\n",
            "H: 17\n",
            "K: 18\n",
            "L: 19\n",
            "M: 20\n",
            "N: 21\n",
            "P: 22\n",
            "Q: 23\n",
            "R: 24\n",
            "S: 25\n",
            "T: 26\n",
            "U: 27\n",
            "V: 28\n",
            "X: 29\n",
            "Y: 30\n",
            "a: 31\n",
            "b: 32\n",
            "c: 33\n",
            "d: 34\n",
            "e: 35\n",
            "g: 36\n",
            "h: 37\n",
            "i: 38\n",
            "k: 39\n",
            "l: 40\n",
            "m: 41\n",
            "n: 42\n",
            "o: 43\n",
            "p: 44\n",
            "q: 45\n",
            "r: 46\n",
            "s: 47\n",
            "t: 48\n",
            "u: 49\n",
            "v: 50\n",
            "x: 51\n",
            "y: 52\n",
            "À: 53\n",
            "Á: 54\n",
            "Â: 55\n",
            "Ô: 56\n",
            "Ý: 57\n",
            "à: 58\n",
            "á: 59\n",
            "â: 60\n",
            "ã: 61\n",
            "è: 62\n",
            "é: 63\n",
            "ê: 64\n",
            "ì: 65\n",
            "í: 66\n",
            "ò: 67\n",
            "ó: 68\n",
            "ô: 69\n",
            "õ: 70\n",
            "ù: 71\n",
            "ú: 72\n",
            "ý: 73\n",
            "Ă: 74\n",
            "ă: 75\n",
            "Đ: 76\n",
            "đ: 77\n",
            "ĩ: 78\n",
            "ũ: 79\n",
            "Ơ: 80\n",
            "ơ: 81\n",
            "Ư: 82\n",
            "ư: 83\n",
            "ạ: 84\n",
            "Ả: 85\n",
            "ả: 86\n",
            "ấ: 87\n",
            "ầ: 88\n",
            "Ẩ: 89\n",
            "ẩ: 90\n",
            "ẫ: 91\n",
            "ậ: 92\n",
            "ắ: 93\n",
            "ằ: 94\n",
            "ẳ: 95\n",
            "ẵ: 96\n",
            "ặ: 97\n",
            "ẹ: 98\n",
            "ẻ: 99\n",
            "ẽ: 100\n",
            "ế: 101\n",
            "ề: 102\n",
            "ể: 103\n",
            "ễ: 104\n",
            "ệ: 105\n",
            "ỉ: 106\n",
            "ị: 107\n",
            "ọ: 108\n",
            "ỏ: 109\n",
            "Ố: 110\n",
            "ố: 111\n",
            "Ồ: 112\n",
            "ồ: 113\n",
            "ổ: 114\n",
            "ỗ: 115\n",
            "ộ: 116\n",
            "ớ: 117\n",
            "ờ: 118\n",
            "Ở: 119\n",
            "ở: 120\n",
            "ỡ: 121\n",
            "ợ: 122\n",
            "ụ: 123\n",
            "ủ: 124\n",
            "ứ: 125\n",
            "Ừ: 126\n",
            "ừ: 127\n",
            "ử: 128\n",
            "ữ: 129\n",
            "ự: 130\n",
            "ỳ: 131\n",
            "ỵ: 132\n",
            "ỷ: 133\n",
            "ỹ: 134\n",
            "\n",
            "Bộ giải mã (Decoder):\n",
            "0:  \n",
            "1: !\n",
            "2: \"\n",
            "3: '\n",
            "4: ,\n",
            "5: -\n",
            "6: .\n",
            "7: :\n",
            "8: ;\n",
            "9: ?\n",
            "10: A\n",
            "11: B\n",
            "12: C\n",
            "13: D\n",
            "14: E\n",
            "15: F\n",
            "16: G\n",
            "17: H\n",
            "18: K\n",
            "19: L\n",
            "20: M\n",
            "21: N\n",
            "22: P\n",
            "23: Q\n",
            "24: R\n",
            "25: S\n",
            "26: T\n",
            "27: U\n",
            "28: V\n",
            "29: X\n",
            "30: Y\n",
            "31: a\n",
            "32: b\n",
            "33: c\n",
            "34: d\n",
            "35: e\n",
            "36: g\n",
            "37: h\n",
            "38: i\n",
            "39: k\n",
            "40: l\n",
            "41: m\n",
            "42: n\n",
            "43: o\n",
            "44: p\n",
            "45: q\n",
            "46: r\n",
            "47: s\n",
            "48: t\n",
            "49: u\n",
            "50: v\n",
            "51: x\n",
            "52: y\n",
            "53: À\n",
            "54: Á\n",
            "55: Â\n",
            "56: Ô\n",
            "57: Ý\n",
            "58: à\n",
            "59: á\n",
            "60: â\n",
            "61: ã\n",
            "62: è\n",
            "63: é\n",
            "64: ê\n",
            "65: ì\n",
            "66: í\n",
            "67: ò\n",
            "68: ó\n",
            "69: ô\n",
            "70: õ\n",
            "71: ù\n",
            "72: ú\n",
            "73: ý\n",
            "74: Ă\n",
            "75: ă\n",
            "76: Đ\n",
            "77: đ\n",
            "78: ĩ\n",
            "79: ũ\n",
            "80: Ơ\n",
            "81: ơ\n",
            "82: Ư\n",
            "83: ư\n",
            "84: ạ\n",
            "85: Ả\n",
            "86: ả\n",
            "87: ấ\n",
            "88: ầ\n",
            "89: Ẩ\n",
            "90: ẩ\n",
            "91: ẫ\n",
            "92: ậ\n",
            "93: ắ\n",
            "94: ằ\n",
            "95: ẳ\n",
            "96: ẵ\n",
            "97: ặ\n",
            "98: ẹ\n",
            "99: ẻ\n",
            "100: ẽ\n",
            "101: ế\n",
            "102: ề\n",
            "103: ể\n",
            "104: ễ\n",
            "105: ệ\n",
            "106: ỉ\n",
            "107: ị\n",
            "108: ọ\n",
            "109: ỏ\n",
            "110: Ố\n",
            "111: ố\n",
            "112: Ồ\n",
            "113: ồ\n",
            "114: ổ\n",
            "115: ỗ\n",
            "116: ộ\n",
            "117: ớ\n",
            "118: ờ\n",
            "119: Ở\n",
            "120: ở\n",
            "121: ỡ\n",
            "122: ợ\n",
            "123: ụ\n",
            "124: ủ\n",
            "125: ứ\n",
            "126: Ừ\n",
            "127: ừ\n",
            "128: ử\n",
            "129: ữ\n",
            "130: ự\n",
            "131: ỳ\n",
            "132: ỵ\n",
            "133: ỷ\n",
            "134: ỹ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "    # Thử nghiệm với một tệp mới\n",
        "    test_file = audio_files[99]\n",
        "    test_features = extract_features(test_file)\n",
        "\n",
        "    # Dự đoán câu tương ứng\n",
        "    if test_features is not None:\n",
        "        best_score = float(\"-inf\")\n",
        "        best_transcript = None\n",
        "        for token_ids, model in models.items():\n",
        "            try:\n",
        "                score = model.score(test_features)\n",
        "                if score > best_score:\n",
        "                    best_score = score\n",
        "                    best_transcript = token_ids\n",
        "            except Exception as e:\n",
        "                print(f\"Lỗi khi tính điểm với HMM: {e}\")\n",
        "                continue\n",
        "\n",
        "        if best_transcript:\n",
        "            predicted_sentence = tokenizer.decode(list(best_transcript))\n",
        "            print(f\"Dự đoán: {predicted_sentence}\")\n",
        "        else:\n",
        "            print(\"Không tìm thấy câu phù hợp.\")\n",
        "    else:\n",
        "        print(\"Không thể trích xuất đặc trưng từ tệp thử nghiệm.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C_OVOAH0DkoM",
        "outputId": "378ce2dd-97a2-4400-a9f9-f1d98e36ee53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dự đoán: Ông bà tao ngày ấy thì sợ lắm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JMUN-G_iEh2G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}